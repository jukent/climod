R
library(ncdf4)
library(KernSmooth)
library(devtools)
load_all()
check()

## To generate / update man files
## (rm NAMESPACE to regenerate it)
library(roxygen2)
roxygenize()

## Run from parent directory to generate docco PDF
R CMD Rd2pdf climod



# INTEGRATION TEST:

(Consider making into unit test - then it'll run automatically with check())

R -f bias-correct.R

cd tests
foreach i (*nc)
ncdump $i > foo
ncdump check/$i > bar
echo $i
diff foo bar
echo =================
rm foo bar
end
cd ..


Argh, some change has snuck in here without me noticing.  Must see if
I can track it down...

##########################

CODE:

# fshatter / fmeld
# 	split netcdf file into .Rdata rods/sheets for parallel processing
# 	recompose in memory

nc_scaffold [monkeypatch]

? xyapply

[bc script]


DOCCO:

biascorrect

stub examples:
     slice
     nc_history
     nc_ingest

[netcdf files can't go in data/ folder of package; full examples that
use netcdf data need to go in a vignette (q.v. below)]


NEW FEATURES:

dependency on ncdf4?  Examples for nc_ functions?


pdf vs pdf plot
	PDF A up
	PDF B down
	overlay A-B (= delta between identity and xfer f'n?)
	rugs

add plot, print methods for cslice
for plot.cslice - 2 lists -> matrices (pad w/NA) for mplot
# plot.cslice <- function(cs, inner.args=NULL, ...){
#     itime = slice(cs$time, cs, outer=FALSE)
#     otime = slice(cs$time, cs, outer=TRUE)
#     mplot(otime, cs$outer, pch=pcho, ...)
#     mapply(points, itime, cs$inner, inner.args)
# }

mplot: make it work on lists of things with different lengths, and in
the absence of x & y members.  What I want is something that acts like
I called "plot" on each list element, but plots them all on a common
field.  (And uses matplot to be sensible about multiple overlays.) 

existing mplot -> mxyplot?
mplot for more general plot of 2 lists?

detrend [all normalization params]
	exploit cyclic nature of data - param by year + fit
	more than first-order?



OPEN ISSUES:

rename:  distmap -> kddm

namelist(obs,cur,fut) -> class bcdata

? fit lambda instead of 0.25 for power norm?
figure out what to do with denormalize when power is different

avoid overfitting of xfer f'n:
	splinefun -> smooth.spline? [need to guarantee monotonicity]
	thin KDE inputs to splinefun?  
 

UNIT TESTS:

# dedrizzle
denormalize, normalize
unzero

attributes: atsign, copyatts

slicing: cslice, slice

biascorrect

distmap
pdf2cdf
predict.distmap

nc_history
nc_ingest

renest

## too hard to test?
pdfskill
tailskill


untested (trivial): namelist, yearlength
untested (graphics): mplot, plot.distmap




###########################
====== FOR VIGNETTE =======
###########################




## Inner slice example: separate data into 12 "monthly" windows
## Note: example data comes from southern hemisphere
library(ncdf4)
 
nc <- nc_ingest("tests/raw/tmax.cur.nc")

tmax <- nc$tmax[1,1,]
time <- nc$time

monthly <- cslice(time, num=12, ratio=1, names=month.abb)

mondata <- slice(tmax, monthly)
montime <- slice(time, monthly)

## Plot data colored by monthly window
par(bg="gray")
xr <- c(0,365*2)
yr <- range(unlist(tmax))
col <- topo.colors(12); names(col) <- month.abb
plot(NA, xlim=xr, ylim=yr, xlab="day", ylab="tmax")
for(i in month.abb){
    points(montime[[i]],mondata[[i]],col=col[i])
}
legend("topright",names(col),col=col, pch=1, ncol=4, cex=0.75)



## Outer slice example, continuing from above: compare normalizing
## daily data based on 30-day moving climatological window vs
## normalizing using monthly window

daily <- cslice(time, inner=1, outer=30)
daydata <- slice(tmax, daily, outer=TRUE)

dev.new()
par(mfrow=c(3,1))

boxplot(mondata, pch='.', boxwex=1, outline=FALSE, main="Monthly window")
boxplot(daydata, pch='.', boxwex=1, outline=FALSE, main="30-day moving window",
        whisklty=0, boxlty=0, boxfill="gray", xlab="day of year")


daynorm <- lapply(daydata, normalize)
dayanom <- unslice(daynorm, daily)

monnorm <- lapply(mondata, normalize)
monanom <- unslice(monnorm, monthly)

doy <- time %% 365.25

plot(doy, dayanom-monanom, pch='.', main="Anomaly difference", ylab="")
abline(v=seq(0,365.25,len=13),col="gray")





###########################
====== FOR MULTIVAR =======
###########################

I think I can use multivar BC to fill in missing data:

First, do the normalization, kddm construction, and covariance matrix
setup using na.rm=TRUE.

Then set NA to 0 (for two-tailed) or mode(?) for one-tailed and
proceed.  NA becomes a climatology guess, basically, and then gets
adjusted based on what all the other vars are doing.

Obviously, I should test this with synthetic data.


#########################
======= FOR LATER =======
#########################

Default qqplot() function in R uses approx() when vectors are not
identical in size.  That's misleading for plot.distmap, I think.

Create nearest-order-statistic function:
	x = values, p = probabilities,
       	n  = length(x)-1
	xs=sort(x)
	return(xs[1+round(p*n)])

Create q-q plot function that uses nearest-order-statistic.
default number of points equal to smaller of nx, ny
option: multiple samples + jitter, to make a cloud?
like plot.xy -- adds points, doesn't make entire plot

Revise plot.distmap to use the new q-q plot function

Allow the option of skipping the points (new qqplot)

Default xlab = deparse(substitute(object$x)) ?

#########################

Residuals-vs-fit type plot of transfer function?  Deviation from
identity line.

#########################

Real data instead of synthetic for distmap examples?




################################################
=== DOCUMENT SOMEWHERE ZERO-HANDLING PROCESS ===
################################################

set threshold using entire dataset before doing anything else*
drop zeros when constructing distmap
in predict, conserve zeros
check for negatives between predict & denormalize
is that everything?

*rationale: we do thresholding to correct the wet/dry frequency.
 Generally, climate models exhibit excess drizzle, and depending on
 how the output was post-processed, you can have very very small or
 even negative values that will throw off the precipitation frequency
 and the distribution mapping.  Although the wet/dry frequency has
 seasonality, the drizzle problem is more about representation than it
 is about model dynamics, so the cutoff threshold should be pretty
 constant in time.  Plus, in arid regions, the number of wet days can
 be very low in certain seasons, making it difficult or impossible to
 estimate a time-varying drizzle cutoff.  Therefore, the simplest and
 most appropriate way to adjust the probability of precipitation to
 compensate for the drizzle problem in model output is: (1) floor all
 datasets at zero; (2) calculate the wet/dry fraction based on the
 entire timeseries of observational data; (3) sort the model output
 for the corresponding current period; (4) use the calculated wet-dry
 fraction to find a threshold value in the model output that will
 equalize the wet days in the model with observations (note: this only
 works if there's an excess of wet days in the model; if there's an
 excess of dry days, the threshold will be zero and a univariate bias
 correction cannot correct the wet/dry fraction); (5) set all values
 in all model runs (current and future) below the threshold to zero.

