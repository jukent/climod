R
library(ncdf4)
library(KernSmooth)
library(devtools)
load_all()
check()

## To generate / update man files
## (rm NAMESPACE to regenerate it)
library(roxygen2)
roxygenize()

## Run from parent directory to generate docco PDF
R CMD Rd2pdf climod



# INTEGRATION TEST:

(Consider making into unit test - then it'll run automatically with check())

cd tests

R -f full-bc-integration-test.R
foreach i (*nc)
ncdump $i > foo
ncdump check/$i > bar
echo $i
diff foo bar
echo =================
rm foo bar
end
cd ..


Argh, some change has snuck in here without me noticing.  Must see if
I can track it down...

##########################

#####
VIGNETTES:

Minimal changes to code - just get it CRAN-worthy

In the temp example, dive into the details of biascorrect() 
In the prec example, abstract up higher

Need a CORDEX-based example that does full-transient with trend


#####
CODE:

copyatts:
	NULL values in e.g. dimnames causes atts to be lost on []
	update nc_ingest() to fill dimnames with string(coord vars)
	       need att to track original type (float, double, etc.)
	skip dimnames on copyatts() if dimensions don't match

#####
DOCUMENTATION:

biascorrect

stub examples:
     slice
     nc_history
     nc_ingest

[netcdf files can't go in data/ folder of package; full examples that
use netcdf data need to go in a vignette (q.v. below)]


#####
NEW FEATURES:

For CORDEX, need to pull out the trend rather than the mean
	fit trend with a 2nd-order curve
	fit trend to cur+fut, not separately
	match mean of cur to mean of obs
	but don't do any other adjustments of trend

OLD: detrend [all normalization params]
	exploit cyclic nature of data - param by year + fit
	more than first-order?


Once nc_ingest is linking coord vars to data via (string) dimnames:
	* overload [] with arg coordinate=TRUE to do coord range subset
	* do better netcdf output
	   	is internal nc object representation close enough to
		netcdf data model that nc_scaffold (monkeypatch) is
		unneeded? 


pdf vs pdf plot
	PDF A up
	PDF B down
	overlay A-B (= delta between identity and xfer f'n?)
	rugs

add plot, print methods for cslice
for plot.cslice - 2 lists -> matrices (pad w/NA) for mplot
# plot.cslice <- function(cs, inner.args=NULL, ...){
#     itime = slice(cs$time, cs, outer=FALSE)
#     otime = slice(cs$time, cs, outer=TRUE)
#     mplot(otime, cs$outer, pch=pcho, ...)
#     mapply(points, itime, cs$inner, inner.args)
# }

mplot: make it work on lists of things with different lengths, and in
the absence of x & y members.  What I want is something that acts like
I called "plot" on each list element, but plots them all on a common
field.  (And uses matplot to be sensible about multiple overlays.) 

existing mplot -> mxyplot?
mplot for more general plot of 2 lists?

xyapply?

#####
OPEN ISSUES:

rename:  distmap -> kddm

namelist(obs,cur,fut) -> class bcdata

? fit lambda instead of 0.25 for power norm?
figure out what to do with denormalize when power is different

avoid overfitting of xfer f'n:
	splinefun -> smooth.spline? [need to guarantee monotonicity]
	thin KDE inputs to splinefun?  
 

#####
UNIT TESTS:

# dedrizzle
denormalize, normalize
unzero

attributes: atsign, copyatts

slicing: cslice, slice

biascorrect

distmap
pdf2cdf
predict.distmap

nc_history
nc_ingest

renest

## too hard to test?
pdfskill
tailskill


untested (trivial): namelist, yearlength
untested (graphics): mplot, plot.distmap





###########################
====== FOR MULTIVAR =======
###########################

I think I can use multivar BC to fill in missing data:

First, do the normalization, kddm construction, and covariance matrix
setup using na.rm=TRUE.

Then set NA to 0 (for two-tailed) or mode(?) for one-tailed and
proceed.  NA becomes a climatology guess, basically, and then gets
adjusted based on what all the other vars are doing.

Obviously, I should test this with synthetic data.


#########################
======= FOR LATER =======
#########################

Default qqplot() function in R uses approx() when vectors are not
identical in size.  That's misleading for plot.distmap, I think.

Create nearest-order-statistic function:
	x = values, p = probabilities,
       	n  = length(x)-1
	xs=sort(x)
	return(xs[1+round(p*n)])

Create q-q plot function that uses nearest-order-statistic.
default number of points equal to smaller of nx, ny
option: multiple samples + jitter, to make a cloud?
like plot.xy -- adds points, doesn't make entire plot

Revise plot.distmap to use the new q-q plot function

Allow the option of skipping the points (new qqplot)

Default xlab = deparse(substitute(object$x)) ?

#########################

Residuals-vs-fit type plot of transfer function?  Deviation from
identity line.

#########################

Real data instead of synthetic for distmap examples?




################################################
=== DOCUMENT SOMEWHERE ZERO-HANDLING PROCESS ===
################################################

set threshold using entire dataset before doing anything else*
drop zeros when constructing distmap
in predict, conserve zeros
check for negatives between predict & denormalize
is that everything?

*rationale: we do thresholding to correct the wet/dry frequency.
 Generally, climate models exhibit excess drizzle, and depending on
 how the output was post-processed, you can have very very small or
 even negative values that will throw off the precipitation frequency
 and the distribution mapping.  Although the wet/dry frequency has
 seasonality, the drizzle problem is more about representation than it
 is about model dynamics, so the cutoff threshold should be pretty
 constant in time.  Plus, in arid regions, the number of wet days can
 be very low in certain seasons, making it difficult or impossible to
 estimate a time-varying drizzle cutoff.  Therefore, the simplest and
 most appropriate way to adjust the probability of precipitation to
 compensate for the drizzle problem in model output is: (1) floor all
 datasets at zero; (2) calculate the wet/dry fraction based on the
 entire timeseries of observational data; (3) sort the model output
 for the corresponding current period; (4) use the calculated wet-dry
 fraction to find a threshold value in the model output that will
 equalize the wet days in the model with observations (note: this only
 works if there's an excess of wet days in the model; if there's an
 excess of dry days, the threshold will be zero and a univariate bias
 correction cannot correct the wet/dry fraction); (5) set all values
 in all model runs (current and future) below the threshold to zero.

